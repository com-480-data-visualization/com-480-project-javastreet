{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To scrape the data we will use BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(\"http\", 'html.parser')\n",
    "\n",
    "#Let's start on the main page of the website\n",
    "URL = 'https://www.androidrank.org/android-most-popular-google-play-apps'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Initialization of an empty dataframe with the desired columns (URL is ou unique ID)\n",
    "new_columns = [\"URL\", 'Rank','Name', 'Nb_Rating', 'Installs', 'Avg_Rating', \"Ev_30\", \"Ev_60\", \"Price\", \"Category\"]\n",
    "app_pd = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "# We go throught all the categories and take all the given app and their ranking in that category\n",
    "nav = soup.find(\"nav\")\n",
    "start = nav.find(\"b\", text=\"Comics\") #1\n",
    "for b in start.find_next_siblings(\"b\"):\n",
    "    a = b.find(\"a\")\n",
    "    Category = a.get_text()\n",
    "    \n",
    "    # Now we move on the webpage of the desired category\n",
    "    URL = 'https://www.androidrank.org/'+a[\"href\"]\n",
    "    r = requests.get(URL)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    # We select all the app on the webpage with all their informations\n",
    "    table = soup.find(\"table\", id=\"ranklist\")\n",
    "    apps = table.find_all(\"tr\")[1:]\n",
    "    for app in apps:\n",
    "        \n",
    "        # Ev_30 and Ev_60 corresponds to the growth of the number of ratings over the 30 and 60 respectively\n",
    "        URL, Rank, Name, Nb_Rating, Installs, Avg_Rating, Ev_30, Ev_60, Price = '','','','','','','','',''\n",
    "        p = app.find_all(\"td\")\n",
    "        Rank = p[0].text\n",
    "        URL = p[1].find('a', href=True)[\"href\"]\n",
    "        Name = p[1].find('a', href=True).text\n",
    "        Nb_Rating = p[3].text\n",
    "        Installs = p[4].text\n",
    "        Avg_Rating = p[5].text\n",
    "        Ev_30 = p[6].text\n",
    "        Ev_60 = p[7].text\n",
    "        Price = p[8].text\n",
    "        \n",
    "        # We create a \"dummy\" dataframe to organize the collected data\n",
    "        characteristics_pd = pd.DataFrame([[URL, Rank, Name, Nb_Rating, Installs, Avg_Rating,\n",
    "                                            Ev_30, Ev_60, Price,Category]], columns = new_columns)\n",
    "        \n",
    "        # We append this dataframe to the global dataframe\n",
    "        app_pd = app_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "    #we get the the URL for the next page in the same category\n",
    "    nextpage = soup.find_all('small')[1].find(\"a\", text=\"Next >\")\n",
    "    \n",
    "    # We continue this until there is no next page in the given category anymore\n",
    "    while(len(nextpage['href'])):\n",
    "        \n",
    "        urlnext_page = \"https://www.androidrank.org\" + nextpage['href']\n",
    "        r = requests.get(urlnext_page)\n",
    "        page_body = r.text\n",
    "        soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "        table = soup.find(\"table\", id=\"ranklist\")\n",
    "        apps = table.find_all(\"tr\")[1:]\n",
    "\n",
    "        for app in apps:\n",
    "            URL, Rank, Name, Nb_Rating, Installs, Avg_Rating, Ev_30, Ev_60, Price = '','','','','','','','',''\n",
    "            p = app.find_all(\"td\")\n",
    "            Rank = p[0].text\n",
    "            URL = p[1].find('a', href=True)[\"href\"]\n",
    "            Name = p[1].find('a', href=True).text\n",
    "            Nb_Rating = p[3].text\n",
    "            Installs = p[4].text\n",
    "            Avg_Rating = p[5].text\n",
    "            Ev_30 = p[6].text\n",
    "            Ev_60 = p[7].text\n",
    "            Price = p[8].text\n",
    "            characteristics_pd = pd.DataFrame([[URL, Rank, Name, Nb_Rating, Installs, Avg_Rating,\n",
    "                                                Ev_30, Ev_60, Price, Category]], columns = new_columns)\n",
    "            app_pd = app_pd.append(characteristics_pd,ignore_index=True)\n",
    "            \n",
    "            # To avoid a crash due to the return of a None when there is no next page we manually set it to \"\"\n",
    "            if(soup.find_all('small')[1].find(\"a\", text=\"Next >\")):\n",
    "                nextpage = soup.find_all('small')[1].find(\"a\", text=\"Next >\")\n",
    "            else:\n",
    "                nextpage[\"href\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the excess of requests, we had to relaunch the previous cell and modify line #1 to restart from the desired category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first csv containing data\n",
    "df_app = pd.read_csv('data/1.csv', index_col=[0])\n",
    "\n",
    "# Iterate through the 7 remaining files\n",
    "for i in range(2,9):\n",
    "    df_temp = pd.read_csv('data/' + str(i) + '.csv', index_col=[0])\n",
    "    df_app = pd.concat([df_app, df_temp], ignore_index=True, join='inner')\n",
    "    \n",
    "# Dropping duplicates since the parsing was sometimes stopped during the analysis of one category of apps\n",
    "df_app.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At the end we have the following dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('URL: ' + str(df_app.URL.isnull().values.any()))\n",
    "print('Rank: ' + str(df_app.Rank.isnull().values.any()))\n",
    "print('Name: ' + str(df_app.Name.isnull().values.any()))\n",
    "print('Nb_Rating: ' + str(df_app.Nb_Rating.isnull().values.any()))\n",
    "print('Installs: ' + str(df_app.Installs.isnull().values.any()))\n",
    "print('Avg_Rating: ' + str(df_app.Avg_Rating.isnull().values.any()))\n",
    "print('Ev_30: ' + str(df_app.Ev_30.isnull().values.any()))\n",
    "print('Ev_60: ' + str(df_app.Ev_60.isnull().values.any()))\n",
    "print('Price: ' + str(df_app.Price.isnull().values.any()))\n",
    "print('Category: ' + str(df_app.Category.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The only column with null values is the Category. It corresponds to the global ranking of all apps that were parsed, where the category was not mentionned. Since these apps already appear in their own category (if they are in the global top 500, they are also in their own category top 500), they are duplicates. So let's drop them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.dropna(inplace=True)\n",
    "df_app.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Isn't it weird that we don't have 24 500 rows since we have a top 500 in each category and 49 categories?**\n",
    "**Let's have a closer look**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.Category.value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By looking on the website, we see that this is not a scraping problem but that some categories have less app in the Top. This is already a good thing to highlights the difference of diversity between each categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Little bit of cleaning let's convert back the number from string to int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app[\"Nb_Rating\"] = df_app[\"Nb_Rating\"].str.replace(',', '')\n",
    "df_app[\"Nb_Rating\"] = df_app[\"Nb_Rating\"].astype(int)\n",
    "df_app[\"Installs\"] = df_app[\"Installs\"].str.replace(' ', '')\n",
    "df_app[\"Installs\"] = (df_app[\"Installs\"].str.replace(r'[kM]+$', '', regex=True).astype(float) * \\\n",
    "                        df_app[\"Installs\"].str.extract(r'[\\d\\.]+([kM]+)', expand=False)\n",
    "                         .fillna(1)\n",
    "                          .replace(['k','M'], [10**3, 10**6]).astype(int))\n",
    "df_app[\"Avg_Rating\"] = df_app[\"Avg_Rating\"].astype(float)\n",
    "df_app[\"Ev_30\"] = df_app[\"Ev_30\"].str.replace('%', '').astype(float)\n",
    "df_app[\"Ev_60\"] = df_app[\"Ev_60\"].str.replace('%', '').astype(float)\n",
    "df_app.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We save the cleaned dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.to_csv(\"data/final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We found a new API that gives us even more information about the app such as the developper, whether the app contains add or not so let's use it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import app\n",
    "\n",
    "#It works as follow \n",
    "\n",
    "result = app(\n",
    "    'com.facebook.katana',\n",
    "    lang='fr', # defaults to 'en'\n",
    "    country='fr' # defaults to 'us'\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We add new columns to our datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app[\"histogram\"] = np.empty((len(df_app), 0)).tolist()\n",
    "df_app[\"size\"] = np.nan\n",
    "df_app[\"androidVersion\"] = np.nan\n",
    "df_app[\"developer\"] = np.nan\n",
    "df_app[\"developerId\"] = np.nan\n",
    "df_app[\"containsAds\"] = np.nan\n",
    "df_app[\"released\"] = np.nan\n",
    "df_app[\"updated\"] = np.nan\n",
    "df_app[\"version\"] = np.nan\n",
    "df_app[\"comments\"] = np.empty((len(df_app), 0)).tolist()\n",
    "df_app[\"icon\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First let's see which app are not from the us store to avoid crashing in the API due to the 404 not found**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The variable allows us to have a countdown and also to start back from where we stoped\n",
    "i = 0\n",
    "tot_page = 24500\n",
    "for url in df_app[\"URL\"][i:]:\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    codename = url.split('/')[-1]\n",
    "    url_search = 'https://play.google.com/store/apps/details?id='+codename+ '&hl=en&gl=us'\n",
    "    r=requests.get(url_search)\n",
    "    if r.status_code == 404:\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"Parsable\"]= False\n",
    "    else:\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"Parsable\"]= True\n",
    "\n",
    "\n",
    "indexs = df_app[df_app['Parsable'] == False ].index\n",
    " \n",
    "# Delete these row that are not in the us store indexes from dataFrame\n",
    "df_app.drop(indexs , inplace=True)\n",
    "df_app.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we add all the desired information except for the comments that will be scraped only for the most rated app**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "tot_page = 25636\n",
    "for url in df_app[\"URL\"]:\n",
    "    i += 1\n",
    "    printed= i/tot_page*100\n",
    "    stdout.write(\"\\r%f %%\" % printed)\n",
    "    stdout.flush()\n",
    "    if df_app.loc[df_app[\"URL\"]==url,\"Parsable\"].values[0]:\n",
    "        codename = url.split('/')[-1]\n",
    "        result = app(\n",
    "            codename,\n",
    "            lang='en', \n",
    "            country='us' \n",
    "        )\n",
    "        df_app.at[df_app.index[df_app[\"URL\"]==url].tolist()[0],\"histogram\"] = result[\"histogram\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"size\"] = result[\"size\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"androidVersion\"] = result[\"androidVersion\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"developer\"] = result[\"developer\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"developerId\"] = result[\"developerId\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"containsAds\"] = result[\"containsAds\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"released\"] = result[\"released\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"updated\"] = result[\"updated\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"version\"] = result[\"version\"]\n",
    "        df_app.loc[df_app[\"URL\"]==url,\"icon\"] = result[\"icon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL               False\n",
       "Rank              False\n",
       "Name              False\n",
       "Nb_Rating         False\n",
       "Installs          False\n",
       "Avg_Rating        False\n",
       "Ev_30             False\n",
       "Ev_60             False\n",
       "Price             False\n",
       "Category          False\n",
       "Parsable          False\n",
       "histogram          True\n",
       "size              False\n",
       "androidVersion     True\n",
       "developer         False\n",
       "developerId       False\n",
       "containsAds        True\n",
       "released           True\n",
       "updated           False\n",
       "version            True\n",
       "comments           True\n",
       "icon              False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We still have some nan values let's have a look at it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[df_app[\"histogram\"].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[df_app[\"androidVersion\"].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7030, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[df_app[\"containsAds\"].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1689, 22)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[df_app[\"released\"].isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 22)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[df_app[\"version\"].isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that the only significant one that can be due to an error is the containsAds. Let's have a look at it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    15716\n",
       "Name: containsAds, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[\"containsAds\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We see that the nan value corresponds to the False let's change this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.loc[df_app[\"containsAds\"].isna(),\"containsAds\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     15716\n",
       "False     7030\n",
       "Name: containsAds, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_app[\"containsAds\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save this final dataset\n",
    "df_app.to_csv(\"data/final_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
