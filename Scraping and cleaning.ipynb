{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To scrape the data we will use BeautifulSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(\"http\", 'html.parser')\n",
    "\n",
    "#Let's start on the main page of the website\n",
    "URL = 'https://www.androidrank.org/android-most-popular-google-play-apps'\n",
    "r = requests.get(URL)\n",
    "page_body = r.text\n",
    "soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "# Initialization of an empty dataframe with the desired columns (URL is ou unique ID)\n",
    "new_columns = [\"URL\", 'Rank','Name', 'Nb_Rating', 'Installs', 'Avg_Rating', \"Ev_30\", \"Ev_60\", \"Price\", \"Category\"]\n",
    "app_pd = pd.DataFrame(columns=new_columns)\n",
    "\n",
    "# We go throught all the categories and take all the given app and their ranking in that category\n",
    "nav = soup.find(\"nav\")\n",
    "start = nav.find(\"b\", text=\"Comics\") #1\n",
    "for b in start.find_next_siblings(\"b\"):\n",
    "    a = b.find(\"a\")\n",
    "    Category = a.get_text()\n",
    "    \n",
    "    # Now we move on the webpage of the desired category\n",
    "    URL = 'https://www.androidrank.org/'+a[\"href\"]\n",
    "    r = requests.get(URL)\n",
    "    page_body = r.text\n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    # We select all the app on the webpage with all their informations\n",
    "    table = soup.find(\"table\", id=\"ranklist\")\n",
    "    apps = table.find_all(\"tr\")[1:]\n",
    "    for app in apps:\n",
    "        \n",
    "        # Ev_30 and Ev_60 corresponds to the growth of the number of ratings over the 30 and 60 respectively\n",
    "        URL, Rank, Name, Nb_Rating, Installs, Avg_Rating, Ev_30, Ev_60, Price = '','','','','','','','',''\n",
    "        p = app.find_all(\"td\")\n",
    "        Rank = p[0].text\n",
    "        URL = p[1].find('a', href=True)[\"href\"]\n",
    "        Name = p[1].find('a', href=True).text\n",
    "        Nb_Rating = p[3].text\n",
    "        Installs = p[4].text\n",
    "        Avg_Rating = p[5].text\n",
    "        Ev_30 = p[6].text\n",
    "        Ev_60 = p[7].text\n",
    "        Price = p[8].text\n",
    "        \n",
    "        # We create a \"dummy\" dataframe to organize the collected data\n",
    "        characteristics_pd = pd.DataFrame([[URL, Rank, Name, Nb_Rating, Installs, Avg_Rating,\n",
    "                                            Ev_30, Ev_60, Price,Category]], columns = new_columns)\n",
    "        \n",
    "        # We append this dataframe to the global dataframe\n",
    "        app_pd = app_pd.append(characteristics_pd,ignore_index=True)\n",
    "    \n",
    "    #we get the the URL for the next page in the same category\n",
    "    nextpage = soup.find_all('small')[1].find(\"a\", text=\"Next >\")\n",
    "    \n",
    "    # We continue this until there is no next page in the given category anymore\n",
    "    while(len(nextpage['href'])):\n",
    "        \n",
    "        urlnext_page = \"https://www.androidrank.org\" + nextpage['href']\n",
    "        r = requests.get(urlnext_page)\n",
    "        page_body = r.text\n",
    "        soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "        table = soup.find(\"table\", id=\"ranklist\")\n",
    "        apps = table.find_all(\"tr\")[1:]\n",
    "\n",
    "        for app in apps:\n",
    "            URL, Rank, Name, Nb_Rating, Installs, Avg_Rating, Ev_30, Ev_60, Price = '','','','','','','','',''\n",
    "            p = app.find_all(\"td\")\n",
    "            Rank = p[0].text\n",
    "            URL = p[1].find('a', href=True)[\"href\"]\n",
    "            Name = p[1].find('a', href=True).text\n",
    "            Nb_Rating = p[3].text\n",
    "            Installs = p[4].text\n",
    "            Avg_Rating = p[5].text\n",
    "            Ev_30 = p[6].text\n",
    "            Ev_60 = p[7].text\n",
    "            Price = p[8].text\n",
    "            characteristics_pd = pd.DataFrame([[URL, Rank, Name, Nb_Rating, Installs, Avg_Rating,\n",
    "                                                Ev_30, Ev_60, Price, Category]], columns = new_columns)\n",
    "            app_pd = app_pd.append(characteristics_pd,ignore_index=True)\n",
    "            \n",
    "            # To avoid a crash due to the return of a None when there is no next page we manually set it to \"\"\n",
    "            if(soup.find_all('small')[1].find(\"a\", text=\"Next >\")):\n",
    "                nextpage = soup.find_all('small')[1].find(\"a\", text=\"Next >\")\n",
    "            else:\n",
    "                nextpage[\"href\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the excess of requests, we had to relaunch the previous cell and modify line #1 to restart from the desired category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first csv containing data\n",
    "df_app = pd.read_csv('data/1.csv', index_col=[0])\n",
    "\n",
    "# Iterate through the 7 remaining files\n",
    "for i in range(2,9):\n",
    "    df_temp = pd.read_csv('data/' + str(i) + '.csv', index_col=[0])\n",
    "    df_app = pd.concat([df_app, df_temp], ignore_index=True, join='inner')\n",
    "    \n",
    "# Dropping duplicates since the parsing was sometimes stopped during the analysis of one category of apps\n",
    "df_app.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At the end we have the following dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('URL: ' + str(df_app.URL.isnull().values.any()))\n",
    "print('Rank: ' + str(df_app.Rank.isnull().values.any()))\n",
    "print('Name: ' + str(df_app.Name.isnull().values.any()))\n",
    "print('Nb_Rating: ' + str(df_app.Nb_Rating.isnull().values.any()))\n",
    "print('Installs: ' + str(df_app.Installs.isnull().values.any()))\n",
    "print('Avg_Rating: ' + str(df_app.Avg_Rating.isnull().values.any()))\n",
    "print('Ev_30: ' + str(df_app.Ev_30.isnull().values.any()))\n",
    "print('Ev_60: ' + str(df_app.Ev_60.isnull().values.any()))\n",
    "print('Price: ' + str(df_app.Price.isnull().values.any()))\n",
    "print('Category: ' + str(df_app.Category.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The only column with null values is the Category. It corresponds to the global ranking of all apps that were parsed, where the category was not mentionned. Since these apps already appear in their own category (if they are in the global top 500, they are also in their own category top 500), they are duplicates. So let's drop them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.dropna(inplace=True)\n",
    "df_app.isna().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Isn't it weird that we don't have 24 500 rows since we have a top 500 in each category and 49 categories?**\n",
    "**Let's have a closer look**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.Category.value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By looking on the website, we see that this is not a scraping problem but that some categories have less app in the Top. This is already a good thing to highlights the difference of diversity between each categories**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Little bit of cleaning let's convert back the number from string to int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app[\"Nb_Rating\"] = df_app[\"Nb_Rating\"].str.replace(',', '')\n",
    "df_app[\"Nb_Rating\"] = df_app[\"Nb_Rating\"].astype(int)\n",
    "df_app[\"Installs\"] = df_app[\"Installs\"].str.replace(' ', '')\n",
    "df_app[\"Installs\"] = (df_app[\"Installs\"].str.replace(r'[kM]+$', '', regex=True).astype(float) * \\\n",
    "                        df_app[\"Installs\"].str.extract(r'[\\d\\.]+([kM]+)', expand=False)\n",
    "                         .fillna(1)\n",
    "                          .replace(['k','M'], [10**3, 10**6]).astype(int))\n",
    "df_app[\"Avg_Rating\"] = df_app[\"Avg_Rating\"].astype(float)\n",
    "df_app[\"Ev_30\"] = df_app[\"Ev_30\"].str.replace('%', '').astype(float)\n",
    "df_app[\"Ev_60\"] = df_app[\"Ev_60\"].str.replace('%', '').astype(float)\n",
    "df_app.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We save the cleaned dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app.to_csv(\"data/final_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
